{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset\n",
    "1. Vocabulary mapping each word to an index\n",
    "2. Pytorch Dataset to load the data\n",
    "3. Padding of every batch to have fixed sequence length in a given batch and setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy # for tokenization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence # padding of every batch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "        \n",
    "    def __len__(self): return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer_en(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        frequencies = {} # store the frequency of each word encountered\n",
    "        idx = 4 # 0, 1, 2 and 3 are already set\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenizer_en(sentence):\n",
    "                if word not in frequencies: frequencies[word] = 1\n",
    "                else: frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "        \n",
    "    def stoi(self, word): \n",
    "        if word in self.stoi: return self.stoi[word]\n",
    "        else: return \"<UNK>\"\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        if idx in self.itos: return self.itos[idx]\n",
    "        else: return -1\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        tokens = self.tokenizer_en(sentence)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokens \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, root_dir, filename, freq_threshold=1):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(os.path.join(root_dir,filename))\n",
    "        \n",
    "        self.sentiments = self.df[\"sentiment\"]\n",
    "        self.texts = self.df[\"text\"]\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.texts.tolist())\n",
    "        \n",
    "    def __len__(self): return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoded_text = [[self.vocab.stoi[\"<SOS>\"]]]\n",
    "        encoded_text += [[encoded_token] for encoded_token in self.vocab.encode(self.texts[idx])]\n",
    "        encoded_text.append([self.vocab.stoi[\"<EOS>\"]])\n",
    "        \n",
    "        return { \"text\": torch.tensor(encoded_text).float(), \"sentiment\": self.sentiments[idx] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateBatch:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        sentiments = [item[\"sentiment\"] for item in batch]\n",
    "        texts = [item[\"text\"] for item in batch]\n",
    "        texts = pad_sequence(texts, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        return texts, torch.tensor(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir, filename, batch_size=10, num_workers=1, shuffle=True, pin_memory=True):\n",
    "    dataset = SentimentDataset(root_dir, filename)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, \n",
    "                        shuffle=shuffle, pin_memory=pin_memory, collate_fn=CollateBatch(pad_idx=pad_idx))\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, output_size=2):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.wx = nn.Linear(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, x): # x (seq_len, batch_size, input_size)\n",
    "        hidden_state = self.init_hidden_state(batch_size=x.shape[1])\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            hidden_state = torch.sigmoid(self.wh(hidden_state) + self.wx(x[i])) # (batch_size, hidden_size)\n",
    "        \n",
    "        return F.softmax(self.fc(hidden_state), dim=1)\n",
    "    \n",
    "    def overfit(self, text, sentiment, epochs, lr=0.001):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss() # ignore_index=pad_idx?\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            # forward\n",
    "            output = self.forward(text)\n",
    "\n",
    "            loss = criterion(output, sentiment)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
    "\n",
    "            # gradient descent or Adam step\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss\n",
    "            if (epoch % 10 == 0): print(f\"epoch [{epoch+1} / {epochs}] | total loss: {total_loss}\")\n",
    "    \n",
    "    \n",
    "    def fit(self, dataset, batch_size, epochs, lr=0.001):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss() # ignore_index=pad_idx?\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for idx, (texts, sentiments) in enumerate(dataset):\n",
    "                # forward\n",
    "                output = self.forward(texts)\n",
    "                \n",
    "                loss = criterion(output, sentiments)\n",
    "                \n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
    "                \n",
    "                # gradient descent or Adam step\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss\n",
    "            \n",
    "            if (epoch % 10 == 0): print(f\"epoch [{epoch+1} / {epochs}] | total loss: {total_loss}\")\n",
    "    \n",
    "    def get_sentiment(self, vocab, sentence):\n",
    "        encoded_text = [[vocab.stoi[\"<SOS>\"]]]\n",
    "        encoded_text += [[encoded_token] for encoded_token in vocab.encode(sentence)]\n",
    "        encoded_text.append([vocab.stoi[\"<EOS>\"]])\n",
    "        encoded_text = torch.tensor(encoded_text).float()\n",
    "\n",
    "        return classifier(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2\n",
    "HIDDEN_SIZE=64\n",
    "OUTPUT_SIZE=2\n",
    "INPUT_SIZE=1\n",
    "LR=0.1\n",
    "NUM_EPOCHS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentClassifier(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE,\n",
    "                                output_size=OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY OVERFITTING ONE SAMPLE OF DATA\n",
    "text, sentiment = dataset[0].values()\n",
    "sentiment = torch.tensor([sentiment])\n",
    "\n",
    "# print the sentence\n",
    "for i in range(text.shape[0]):\n",
    "    print(dataset.vocab.itos[text[i].item()])\n",
    "\n",
    "# train on this single sentence\n",
    "classifier.overfit(text=text, sentiment=sentiment, epochs=NUM_EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATA\n",
    "dataloader,dataset = get_loader(\"./data/\", \"small_sentiments.csv\", batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1 / 200] | total loss: 3.6414928436279297\n",
      "epoch [11 / 200] | total loss: 3.7530465126037598\n",
      "epoch [21 / 200] | total loss: 3.7530465126037598\n",
      "epoch [31 / 200] | total loss: 3.753046751022339\n",
      "epoch [41 / 200] | total loss: 3.753046751022339\n",
      "epoch [51 / 200] | total loss: 3.7530465126037598\n",
      "epoch [61 / 200] | total loss: 3.7530465126037598\n",
      "epoch [71 / 200] | total loss: 3.7530465126037598\n",
      "epoch [81 / 200] | total loss: 3.7530465126037598\n",
      "epoch [91 / 200] | total loss: 3.7530465126037598\n",
      "epoch [101 / 200] | total loss: 3.7530465126037598\n",
      "epoch [111 / 200] | total loss: 3.753046751022339\n",
      "epoch [121 / 200] | total loss: 3.7530465126037598\n",
      "epoch [131 / 200] | total loss: 3.7530465126037598\n",
      "epoch [141 / 200] | total loss: 3.7530465126037598\n",
      "epoch [151 / 200] | total loss: 3.753046751022339\n",
      "epoch [161 / 200] | total loss: 3.7530465126037598\n",
      "epoch [171 / 200] | total loss: 3.7530465126037598\n",
      "epoch [181 / 200] | total loss: 3.7530465126037598\n",
      "epoch [191 / 200] | total loss: 3.7530465126037598\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "classifier.fit(dataloader, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 2.7247e-17]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_sentiment(dataset.vocab, \"I hate rain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
