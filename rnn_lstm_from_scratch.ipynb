{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset\n",
    "1. Vocabulary mapping each word to an index\n",
    "2. Pytorch Dataset to load the data\n",
    "3. Padding of every batch to have fixed sequence length in a given batch and setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy # for tokenization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence # padding of every batch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "        \n",
    "    def __len__(self): return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer_en(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        frequencies = {} # store the frequency of each word encountered\n",
    "        idx = 4 # 0, 1, 2 and 3 are already set\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenizer_en(sentence):\n",
    "                if word not in frequencies: frequencies[word] = 1\n",
    "                else: frequencies[word] += 1\n",
    "            \n",
    "            if frequencies[word] == self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "        \n",
    "    def stoi(self, word): \n",
    "        if word in self.stoi: return self.stoi[word]\n",
    "        else: return \"<UNK>\"\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        if idx in self.itos: return self.itos[idx]\n",
    "        else: return -1\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        tokens = self.tokenizer_en(sentence)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokens \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, root_dir, filename, freq_threshold=1):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(os.path.join(root_dir,filename))\n",
    "        \n",
    "        self.sentiments = self.df[\"sentiment\"]\n",
    "        self.texts = self.df[\"text\"]\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.texts.tolist())\n",
    "        \n",
    "    def __len__(self): return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoded_text = [[self.vocab.stoi[\"<SOS>\"]]]\n",
    "        encoded_text += [[encoded_token] for encoded_token in self.vocab.encode(self.texts[idx])]\n",
    "        encoded_text.append([self.vocab.stoi[\"<EOS>\"]])\n",
    "        \n",
    "        return { \"text\": torch.tensor(encoded_text).float(), \"sentiment\": self.sentiments[idx] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateBatch:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        sentiments = [item[\"sentiment\"] for item in batch]\n",
    "        texts = [item[\"text\"] for item in batch]\n",
    "        texts = pad_sequence(texts, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        return texts, torch.tensor(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir, filename, batch_size=10, num_workers=1, shuffle=True, pin_memory=True):\n",
    "    dataset = SentimentDataset(root_dir, filename)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, \n",
    "                        shuffle=shuffle, pin_memory=pin_memory, collate_fn=CollateBatch(pad_idx=pad_idx))\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader,dataset = get_loader(\"./data/\", \"small_sentiments.csv\", batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.wx = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.hidden_state = None\n",
    "    \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        self.hidden_state = torch.zeros(batch_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, x): # x (batch_size, input_size)\n",
    "        self.hidden_state = torch.sigmoid(self.wh(self.hidden_state) + self.wx(x)) # (batch_size, hidden_size)\n",
    "        return self.hidden_state\n",
    "    \n",
    "    def fit(self, dataset, batch_size=2, epochs=10, lr=0.001):\n",
    "        self.init_hidden_state(batch_size)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss() # ignore_index=pad_idx?\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for idx, (texts, sentiments) in enumerate(dataset):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                for i in range(texts[0].shape[0]):\n",
    "                    output = self(texts[0][i])\n",
    "                \n",
    "                loss = criterion(output, sentiments)\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss\n",
    "            \n",
    "            print(f\"epoch [{epoch+1} / {epochs}] | total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1828, 0.6293],\n",
       "        [0.0513, 0.3389]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = iter(dataloader)\n",
    "x = items.next()\n",
    "\n",
    "classifier = SentimentClassifier(input_size=1, hidden_size=2) # 2 classes: positive / negative\n",
    "classifier.init_hidden_state(2)\n",
    "\n",
    "for i in range(x[0].shape[0]):\n",
    "    classifier(x[0][i])\n",
    "\n",
    "classifier.hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 2]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-4c675254640e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-142-99c862fdfd91>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, batch_size, epochs, lr)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 2]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "classifier = SentimentClassifier(input_size=1, hidden_size=2)\n",
    "classifier.fit(dataloader, epochs=100, lr=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1828, 0.6293],\n",
       "         [0.0513, 0.3389]], grad_fn=<SigmoidBackward>), tensor([1, 0]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.hidden_state, torch.tensor(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8524, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "nn.CrossEntropyLoss()(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMFromScratch(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(ScratchLSTM, self).__init__()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
