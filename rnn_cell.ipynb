{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy # for tokenization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence # padding of every batch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "        \n",
    "    def __len__(self): return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer_en(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        frequencies = {} # store the frequency of each word encountered\n",
    "        idx = 4 # 0, 1, 2 and 3 are already set\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenizer_en(sentence):\n",
    "                if word not in frequencies: frequencies[word] = 1\n",
    "                else: frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "        \n",
    "    def stoi(self, word): \n",
    "        if word in self.stoi: return self.stoi[word]\n",
    "        else: return \"<UNK>\"\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        if idx in self.itos: return self.itos[idx]\n",
    "        else: return -1\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        tokens = self.tokenizer_en(sentence)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokens \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, root_dir, filename, freq_threshold=1):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(os.path.join(root_dir,filename))\n",
    "        \n",
    "        self.sentiments = self.df[\"sentiment\"]\n",
    "        self.texts = self.df[\"text\"]\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.texts.tolist())\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def __len__(self): return len(self.df)\n",
    "    \n",
    "    def one_hot_tensor(self, idx):\n",
    "        tensor = np.zeros(self.vocab_size)\n",
    "        tensor[idx] = 1\n",
    "        return tensor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoded_text = [self.one_hot_tensor(self.vocab.stoi[\"<SOS>\"])]\n",
    "        encoded_text += [self.one_hot_tensor(encoded_token) \n",
    "                         for encoded_token in self.vocab.encode(self.texts[idx])]\n",
    "        encoded_text.append(self.one_hot_tensor(self.vocab.stoi[\"<EOS>\"]))\n",
    "        \n",
    "        return { \"text\": torch.tensor(encoded_text).float(), \"sentiment\": torch.tensor(self.sentiments[idx]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateBatch:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        sentiments = [item[\"sentiment\"] for item in batch]\n",
    "        texts = [item[\"text\"] for item in batch]\n",
    "        texts = pad_sequence(texts, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        return texts, torch.tensor(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir, filename, batch_size=10, num_workers=1, shuffle=True, pin_memory=True):\n",
    "    dataset = SentimentDataset(root_dir, filename)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, \n",
    "                        shuffle=shuffle, pin_memory=pin_memory, collate_fn=CollateBatch(pad_idx=pad_idx))\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.whh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.wxh = nn.Linear(input_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state): # x (batch_size, input_size)\n",
    "        return torch.sigmoid(self.whh(hidden_state) + self.wxh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, output_size=2):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = RNNCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state): # x (seq_len, batch_size, input_size)\n",
    "        for i in range(x.shape[0]):\n",
    "            hidden_state = self.rnn(x[i], hidden_state)\n",
    "        return F.softmax(self.fc(hidden_state), dim=1)\n",
    "    \n",
    "    def fit(self, dataset, batch_size, epochs, lr=0.001):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss() # ignore_index=pad_idx?\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for idx, (texts, sentiments) in enumerate(dataset):\n",
    "                # forward\n",
    "                hidden_state = self.init_hidden_state(batch_size=batch_size)\n",
    "                output = self.forward(texts, hidden_state)\n",
    "                \n",
    "                loss = criterion(output, sentiments)\n",
    "                \n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
    "                \n",
    "                # gradient descent or Adam step\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss\n",
    "            \n",
    "            if (epoch % 10 == 0): print(f\"epoch [{epoch+1} / {epochs}] | total loss: {total_loss}\")\n",
    "    \n",
    "    def one_hot_tensor(self, idx, vocab_size):\n",
    "        tensor = [0] * vocab_size\n",
    "        tensor[idx] = 1\n",
    "        return tensor\n",
    "    \n",
    "    def get_sentiment(self, vocab, sentence):\n",
    "        vocab_size = len(vocab)\n",
    "        encoded_text = []\n",
    "        encoded_text.append([self.one_hot_tensor(vocab.stoi[\"<SOS>\"], vocab_size)])\n",
    "        encoded_text += [[self.one_hot_tensor(encoded_token, vocab_size)]\n",
    "                         for encoded_token in vocab.encode(sentence)]\n",
    "        encoded_text.append([self.one_hot_tensor(vocab.stoi[\"<EOS>\"], vocab_size)])\n",
    "        \n",
    "        print(encoded_text)\n",
    "        encoded_text = torch.tensor(encoded_text).float()\n",
    "        print(encoded_text.shape)\n",
    "        h0 = self.init_hidden_state(batch_size=1)\n",
    "        return classifier(encoded_text, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4\n",
    "HIDDEN_SIZE=64\n",
    "OUTPUT_SIZE=2\n",
    "LR=0.1\n",
    "NUM_EPOCHS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATA\n",
    "dataloader,dataset = get_loader(\"./data/\", \"small_sentiments.csv\", batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentClassifier(input_size=dataset.vocab_size, hidden_size=HIDDEN_SIZE,\n",
    "                                output_size=OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1 / 200] | total loss: 1.858147144317627\n",
      "epoch [11 / 200] | total loss: 1.3765232563018799\n",
      "epoch [21 / 200] | total loss: 1.3765232563018799\n",
      "epoch [31 / 200] | total loss: 1.3765232563018799\n",
      "epoch [41 / 200] | total loss: 1.3765232563018799\n",
      "epoch [51 / 200] | total loss: 1.3765232563018799\n",
      "epoch [61 / 200] | total loss: 1.3765232563018799\n",
      "epoch [71 / 200] | total loss: 1.3765232563018799\n",
      "epoch [81 / 200] | total loss: 1.3765232563018799\n",
      "epoch [91 / 200] | total loss: 1.3765232563018799\n",
      "epoch [101 / 200] | total loss: 1.3765232563018799\n",
      "epoch [111 / 200] | total loss: 1.3765232563018799\n",
      "epoch [121 / 200] | total loss: 1.3765232563018799\n",
      "epoch [131 / 200] | total loss: 1.3765232563018799\n",
      "epoch [141 / 200] | total loss: 1.3765232563018799\n",
      "epoch [151 / 200] | total loss: 1.3765232563018799\n",
      "epoch [161 / 200] | total loss: 1.3765232563018799\n",
      "epoch [171 / 200] | total loss: 1.3765232563018799\n",
      "epoch [181 / 200] | total loss: 1.3765232563018799\n",
      "epoch [191 / 200] | total loss: 1.3765233755111694\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "classifier.fit(dataloader, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]\n",
      "torch.Size([5, 1, 34])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7.2347e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_sentiment(dataset.vocab, \"I'm bored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
