{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,y_pred):\n",
    "    \"\"\"MSE\"\"\"\n",
    "    return ((y-y_pred)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,y_pred):\n",
    "    return np.dot(2*x, y_pred-y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5)=0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before training: f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.997, loss = 0.00050331\n",
      "epoch 8: w = 1.999, loss = 0.00001288\n"
     ]
    }
   ],
   "source": [
    "lr=.01\n",
    "n_iter=10\n",
    "for epoch in range(n_iter):\n",
    "    y_pred = forward(X)\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    dw = gradient(X,Y,y_pred)\n",
    "    \n",
    "    w -= lr * dw\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"epoch {epoch}: w = {w:.3f}, loss = {l:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction after training: f(5)=9.999\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction after training: f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do the same thing with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5)=0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before training: f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: w = 0.300, loss = 30.00000000\n",
      "epoch 2: w = 0.772, loss = 15.66018772\n",
      "epoch 4: w = 1.113, loss = 8.17471695\n",
      "epoch 6: w = 1.359, loss = 4.26725292\n",
      "epoch 8: w = 1.537, loss = 2.22753215\n"
     ]
    }
   ],
   "source": [
    "## forward and loss are the same\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    y_pred = forward(X)\n",
    "    l = loss(Y, y_pred)\n",
    "    l.backward() # dl/dw\n",
    "    with torch.no_grad():\n",
    "        w -= lr*w.grad\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"epoch {epoch}: w = {w:.3f}, loss = {l:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction after training: f(5)=8.031\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction after training: f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Pipeline\n",
    "\n",
    "1. Design model (input_size, output_size, froward steps)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    - forward pass: compute predictions\n",
    "    - backward pass: compute gradients\n",
    "    - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 4, features: 1\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(f\"samples: {n_samples}, features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: w = 1.614, loss = 1.13462031\n",
      "epoch 2: w = 1.703, loss = 0.55323672\n",
      "epoch 4: w = 1.765, loss = 0.27323592\n",
      "epoch 6: w = 1.808, loss = 0.13834271\n",
      "epoch 8: w = 1.838, loss = 0.07331493\n",
      "epoch 10: w = 1.859, loss = 0.04192632\n",
      "epoch 12: w = 1.874, loss = 0.02673458\n",
      "epoch 14: w = 1.884, loss = 0.01934218\n",
      "epoch 16: w = 1.892, loss = 0.01570586\n",
      "epoch 18: w = 1.897, loss = 0.01387885\n",
      "epoch 20: w = 1.901, loss = 0.01292392\n",
      "epoch 22: w = 1.904, loss = 0.01238972\n",
      "epoch 24: w = 1.906, loss = 0.01205903\n",
      "epoch 26: w = 1.908, loss = 0.01182712\n",
      "epoch 28: w = 1.909, loss = 0.01164370\n",
      "epoch 30: w = 1.910, loss = 0.01148447\n",
      "epoch 32: w = 1.911, loss = 0.01133772\n",
      "epoch 34: w = 1.912, loss = 0.01119783\n",
      "epoch 36: w = 1.912, loss = 0.01106204\n",
      "epoch 38: w = 1.913, loss = 0.01092905\n",
      "epoch 40: w = 1.914, loss = 0.01079824\n",
      "epoch 42: w = 1.914, loss = 0.01066925\n",
      "epoch 44: w = 1.915, loss = 0.01054192\n",
      "epoch 46: w = 1.915, loss = 0.01041619\n",
      "epoch 48: w = 1.916, loss = 0.01029198\n",
      "epoch 50: w = 1.916, loss = 0.01016928\n",
      "epoch 52: w = 1.917, loss = 0.01004801\n",
      "epoch 54: w = 1.917, loss = 0.00992822\n",
      "epoch 56: w = 1.918, loss = 0.00980985\n",
      "epoch 58: w = 1.918, loss = 0.00969291\n",
      "epoch 60: w = 1.919, loss = 0.00957735\n",
      "epoch 62: w = 1.919, loss = 0.00946318\n",
      "epoch 64: w = 1.920, loss = 0.00935035\n",
      "epoch 66: w = 1.920, loss = 0.00923886\n",
      "epoch 68: w = 1.921, loss = 0.00912872\n",
      "epoch 70: w = 1.921, loss = 0.00901989\n",
      "epoch 72: w = 1.922, loss = 0.00891236\n",
      "epoch 74: w = 1.922, loss = 0.00880610\n",
      "epoch 76: w = 1.923, loss = 0.00870112\n",
      "epoch 78: w = 1.923, loss = 0.00859738\n",
      "epoch 80: w = 1.924, loss = 0.00849488\n",
      "epoch 82: w = 1.924, loss = 0.00839359\n",
      "epoch 84: w = 1.924, loss = 0.00829356\n",
      "epoch 86: w = 1.925, loss = 0.00819468\n",
      "epoch 88: w = 1.925, loss = 0.00809696\n",
      "epoch 90: w = 1.926, loss = 0.00800044\n",
      "epoch 92: w = 1.926, loss = 0.00790505\n",
      "epoch 94: w = 1.927, loss = 0.00781081\n",
      "epoch 96: w = 1.927, loss = 0.00771769\n",
      "epoch 98: w = 1.928, loss = 0.00762567\n",
      "epoch 100: w = 1.928, loss = 0.00753477\n",
      "epoch 102: w = 1.928, loss = 0.00744495\n",
      "epoch 104: w = 1.929, loss = 0.00735620\n",
      "epoch 106: w = 1.929, loss = 0.00726848\n",
      "epoch 108: w = 1.930, loss = 0.00718184\n",
      "epoch 110: w = 1.930, loss = 0.00709620\n",
      "epoch 112: w = 1.931, loss = 0.00701162\n",
      "epoch 114: w = 1.931, loss = 0.00692802\n",
      "epoch 116: w = 1.931, loss = 0.00684543\n",
      "epoch 118: w = 1.932, loss = 0.00676382\n",
      "epoch 120: w = 1.932, loss = 0.00668317\n",
      "epoch 122: w = 1.933, loss = 0.00660350\n",
      "epoch 124: w = 1.933, loss = 0.00652477\n",
      "epoch 126: w = 1.933, loss = 0.00644697\n",
      "epoch 128: w = 1.934, loss = 0.00637013\n",
      "epoch 130: w = 1.934, loss = 0.00629418\n",
      "epoch 132: w = 1.935, loss = 0.00621914\n",
      "epoch 134: w = 1.935, loss = 0.00614502\n",
      "epoch 136: w = 1.935, loss = 0.00607174\n",
      "epoch 138: w = 1.936, loss = 0.00599936\n",
      "epoch 140: w = 1.936, loss = 0.00592783\n",
      "epoch 142: w = 1.936, loss = 0.00585716\n",
      "epoch 144: w = 1.937, loss = 0.00578733\n",
      "epoch 146: w = 1.937, loss = 0.00571834\n",
      "epoch 148: w = 1.938, loss = 0.00565017\n",
      "epoch 150: w = 1.938, loss = 0.00558281\n",
      "epoch 152: w = 1.938, loss = 0.00551624\n",
      "epoch 154: w = 1.939, loss = 0.00545048\n",
      "epoch 156: w = 1.939, loss = 0.00538550\n",
      "epoch 158: w = 1.939, loss = 0.00532130\n",
      "epoch 160: w = 1.940, loss = 0.00525785\n",
      "epoch 162: w = 1.940, loss = 0.00519518\n",
      "epoch 164: w = 1.941, loss = 0.00513323\n",
      "epoch 166: w = 1.941, loss = 0.00507202\n",
      "epoch 168: w = 1.941, loss = 0.00501156\n",
      "epoch 170: w = 1.942, loss = 0.00495182\n",
      "epoch 172: w = 1.942, loss = 0.00489278\n",
      "epoch 174: w = 1.942, loss = 0.00483445\n",
      "epoch 176: w = 1.943, loss = 0.00477682\n",
      "epoch 178: w = 1.943, loss = 0.00471987\n",
      "epoch 180: w = 1.943, loss = 0.00466360\n",
      "epoch 182: w = 1.944, loss = 0.00460800\n",
      "epoch 184: w = 1.944, loss = 0.00455306\n",
      "epoch 186: w = 1.944, loss = 0.00449879\n",
      "epoch 188: w = 1.945, loss = 0.00444515\n",
      "epoch 190: w = 1.945, loss = 0.00439216\n",
      "epoch 192: w = 1.945, loss = 0.00433980\n",
      "epoch 194: w = 1.946, loss = 0.00428804\n",
      "epoch 196: w = 1.946, loss = 0.00423692\n",
      "epoch 198: w = 1.946, loss = 0.00418641\n",
      "epoch 200: w = 1.947, loss = 0.00413651\n",
      "epoch 202: w = 1.947, loss = 0.00408720\n",
      "epoch 204: w = 1.947, loss = 0.00403846\n",
      "epoch 206: w = 1.948, loss = 0.00399033\n",
      "epoch 208: w = 1.948, loss = 0.00394275\n",
      "epoch 210: w = 1.948, loss = 0.00389573\n",
      "epoch 212: w = 1.949, loss = 0.00384930\n",
      "epoch 214: w = 1.949, loss = 0.00380341\n",
      "epoch 216: w = 1.949, loss = 0.00375807\n",
      "epoch 218: w = 1.949, loss = 0.00371326\n",
      "epoch 220: w = 1.950, loss = 0.00366899\n",
      "epoch 222: w = 1.950, loss = 0.00362525\n",
      "epoch 224: w = 1.950, loss = 0.00358203\n",
      "epoch 226: w = 1.951, loss = 0.00353933\n",
      "epoch 228: w = 1.951, loss = 0.00349713\n",
      "epoch 230: w = 1.951, loss = 0.00345543\n",
      "epoch 232: w = 1.952, loss = 0.00341425\n",
      "epoch 234: w = 1.952, loss = 0.00337354\n",
      "epoch 236: w = 1.952, loss = 0.00333332\n",
      "epoch 238: w = 1.952, loss = 0.00329358\n",
      "epoch 240: w = 1.953, loss = 0.00325431\n",
      "epoch 242: w = 1.953, loss = 0.00321551\n",
      "epoch 244: w = 1.953, loss = 0.00317718\n",
      "epoch 246: w = 1.954, loss = 0.00313930\n",
      "epoch 248: w = 1.954, loss = 0.00310187\n",
      "epoch 250: w = 1.954, loss = 0.00306488\n",
      "epoch 252: w = 1.954, loss = 0.00302836\n",
      "epoch 254: w = 1.955, loss = 0.00299224\n",
      "epoch 256: w = 1.955, loss = 0.00295657\n",
      "epoch 258: w = 1.955, loss = 0.00292133\n",
      "epoch 260: w = 1.955, loss = 0.00288651\n",
      "epoch 262: w = 1.956, loss = 0.00285208\n",
      "epoch 264: w = 1.956, loss = 0.00281809\n",
      "epoch 266: w = 1.956, loss = 0.00278449\n",
      "epoch 268: w = 1.956, loss = 0.00275129\n",
      "epoch 270: w = 1.957, loss = 0.00271850\n",
      "epoch 272: w = 1.957, loss = 0.00268608\n",
      "epoch 274: w = 1.957, loss = 0.00265406\n",
      "epoch 276: w = 1.958, loss = 0.00262241\n",
      "epoch 278: w = 1.958, loss = 0.00259115\n",
      "epoch 280: w = 1.958, loss = 0.00256026\n",
      "epoch 282: w = 1.958, loss = 0.00252973\n",
      "epoch 284: w = 1.959, loss = 0.00249957\n",
      "epoch 286: w = 1.959, loss = 0.00246978\n",
      "epoch 288: w = 1.959, loss = 0.00244034\n",
      "epoch 290: w = 1.959, loss = 0.00241124\n",
      "epoch 292: w = 1.959, loss = 0.00238249\n",
      "epoch 294: w = 1.960, loss = 0.00235409\n",
      "epoch 296: w = 1.960, loss = 0.00232603\n",
      "epoch 298: w = 1.960, loss = 0.00229830\n",
      "epoch 300: w = 1.960, loss = 0.00227090\n",
      "epoch 302: w = 1.961, loss = 0.00224383\n",
      "epoch 304: w = 1.961, loss = 0.00221707\n",
      "epoch 306: w = 1.961, loss = 0.00219064\n",
      "epoch 308: w = 1.961, loss = 0.00216453\n",
      "epoch 310: w = 1.962, loss = 0.00213872\n",
      "epoch 312: w = 1.962, loss = 0.00211323\n",
      "epoch 314: w = 1.962, loss = 0.00208802\n",
      "epoch 316: w = 1.962, loss = 0.00206313\n",
      "epoch 318: w = 1.963, loss = 0.00203854\n",
      "epoch 320: w = 1.963, loss = 0.00201424\n",
      "epoch 322: w = 1.963, loss = 0.00199021\n",
      "epoch 324: w = 1.963, loss = 0.00196650\n",
      "epoch 326: w = 1.963, loss = 0.00194305\n",
      "epoch 328: w = 1.964, loss = 0.00191988\n",
      "epoch 330: w = 1.964, loss = 0.00189699\n",
      "epoch 332: w = 1.964, loss = 0.00187438\n",
      "epoch 334: w = 1.964, loss = 0.00185203\n",
      "epoch 336: w = 1.965, loss = 0.00182995\n",
      "epoch 338: w = 1.965, loss = 0.00180814\n",
      "epoch 340: w = 1.965, loss = 0.00178658\n",
      "epoch 342: w = 1.965, loss = 0.00176528\n",
      "epoch 344: w = 1.965, loss = 0.00174423\n",
      "epoch 346: w = 1.966, loss = 0.00172344\n",
      "epoch 348: w = 1.966, loss = 0.00170289\n",
      "epoch 350: w = 1.966, loss = 0.00168259\n",
      "epoch 352: w = 1.966, loss = 0.00166253\n",
      "epoch 354: w = 1.966, loss = 0.00164271\n",
      "epoch 356: w = 1.967, loss = 0.00162313\n",
      "epoch 358: w = 1.967, loss = 0.00160378\n",
      "epoch 360: w = 1.967, loss = 0.00158465\n",
      "epoch 362: w = 1.967, loss = 0.00156577\n",
      "epoch 364: w = 1.967, loss = 0.00154710\n",
      "epoch 366: w = 1.968, loss = 0.00152865\n",
      "epoch 368: w = 1.968, loss = 0.00151043\n",
      "epoch 370: w = 1.968, loss = 0.00149242\n",
      "epoch 372: w = 1.968, loss = 0.00147463\n",
      "epoch 374: w = 1.968, loss = 0.00145705\n",
      "epoch 376: w = 1.969, loss = 0.00143969\n",
      "epoch 378: w = 1.969, loss = 0.00142252\n",
      "epoch 380: w = 1.969, loss = 0.00140555\n",
      "epoch 382: w = 1.969, loss = 0.00138880\n",
      "epoch 384: w = 1.969, loss = 0.00137224\n",
      "epoch 386: w = 1.969, loss = 0.00135588\n",
      "epoch 388: w = 1.970, loss = 0.00133971\n",
      "epoch 390: w = 1.970, loss = 0.00132374\n",
      "epoch 392: w = 1.970, loss = 0.00130796\n",
      "epoch 394: w = 1.970, loss = 0.00129236\n",
      "epoch 396: w = 1.970, loss = 0.00127697\n",
      "epoch 398: w = 1.971, loss = 0.00126174\n",
      "epoch 400: w = 1.971, loss = 0.00124669\n",
      "epoch 402: w = 1.971, loss = 0.00123183\n",
      "epoch 404: w = 1.971, loss = 0.00121715\n",
      "epoch 406: w = 1.971, loss = 0.00120264\n",
      "epoch 408: w = 1.971, loss = 0.00118830\n",
      "epoch 410: w = 1.972, loss = 0.00117413\n",
      "epoch 412: w = 1.972, loss = 0.00116014\n",
      "epoch 414: w = 1.972, loss = 0.00114631\n",
      "epoch 416: w = 1.972, loss = 0.00113264\n",
      "epoch 418: w = 1.972, loss = 0.00111913\n",
      "epoch 420: w = 1.972, loss = 0.00110579\n",
      "epoch 422: w = 1.973, loss = 0.00109261\n",
      "epoch 424: w = 1.973, loss = 0.00107958\n",
      "epoch 426: w = 1.973, loss = 0.00106671\n",
      "epoch 428: w = 1.973, loss = 0.00105399\n",
      "epoch 430: w = 1.973, loss = 0.00104142\n",
      "epoch 432: w = 1.973, loss = 0.00102901\n",
      "epoch 434: w = 1.974, loss = 0.00101674\n",
      "epoch 436: w = 1.974, loss = 0.00100463\n",
      "epoch 438: w = 1.974, loss = 0.00099265\n",
      "epoch 440: w = 1.974, loss = 0.00098082\n",
      "epoch 442: w = 1.974, loss = 0.00096912\n",
      "epoch 444: w = 1.974, loss = 0.00095757\n",
      "epoch 446: w = 1.974, loss = 0.00094615\n",
      "epoch 448: w = 1.975, loss = 0.00093486\n",
      "epoch 450: w = 1.975, loss = 0.00092372\n",
      "epoch 452: w = 1.975, loss = 0.00091271\n",
      "epoch 454: w = 1.975, loss = 0.00090183\n",
      "epoch 456: w = 1.975, loss = 0.00089108\n",
      "epoch 458: w = 1.975, loss = 0.00088045\n",
      "epoch 460: w = 1.976, loss = 0.00086995\n",
      "epoch 462: w = 1.976, loss = 0.00085958\n",
      "epoch 464: w = 1.976, loss = 0.00084934\n",
      "epoch 466: w = 1.976, loss = 0.00083922\n",
      "epoch 468: w = 1.976, loss = 0.00082921\n",
      "epoch 470: w = 1.976, loss = 0.00081932\n",
      "epoch 472: w = 1.976, loss = 0.00080955\n",
      "epoch 474: w = 1.977, loss = 0.00079990\n",
      "epoch 476: w = 1.977, loss = 0.00079036\n",
      "epoch 478: w = 1.977, loss = 0.00078094\n",
      "epoch 480: w = 1.977, loss = 0.00077163\n",
      "epoch 482: w = 1.977, loss = 0.00076243\n",
      "epoch 484: w = 1.977, loss = 0.00075334\n",
      "epoch 486: w = 1.977, loss = 0.00074436\n",
      "epoch 488: w = 1.977, loss = 0.00073549\n",
      "epoch 490: w = 1.978, loss = 0.00072672\n",
      "epoch 492: w = 1.978, loss = 0.00071806\n",
      "epoch 494: w = 1.978, loss = 0.00070950\n",
      "epoch 496: w = 1.978, loss = 0.00070103\n",
      "epoch 498: w = 1.978, loss = 0.00069268\n",
      "epoch 500: w = 1.978, loss = 0.00068442\n",
      "epoch 502: w = 1.978, loss = 0.00067626\n",
      "epoch 504: w = 1.979, loss = 0.00066820\n",
      "epoch 506: w = 1.979, loss = 0.00066023\n",
      "epoch 508: w = 1.979, loss = 0.00065236\n",
      "epoch 510: w = 1.979, loss = 0.00064459\n",
      "epoch 512: w = 1.979, loss = 0.00063690\n",
      "epoch 514: w = 1.979, loss = 0.00062931\n",
      "epoch 516: w = 1.979, loss = 0.00062180\n",
      "epoch 518: w = 1.979, loss = 0.00061439\n",
      "epoch 520: w = 1.980, loss = 0.00060706\n",
      "epoch 522: w = 1.980, loss = 0.00059982\n",
      "epoch 524: w = 1.980, loss = 0.00059268\n",
      "epoch 526: w = 1.980, loss = 0.00058561\n",
      "epoch 528: w = 1.980, loss = 0.00057863\n",
      "epoch 530: w = 1.980, loss = 0.00057173\n",
      "epoch 532: w = 1.980, loss = 0.00056492\n",
      "epoch 534: w = 1.980, loss = 0.00055818\n",
      "epoch 536: w = 1.981, loss = 0.00055153\n",
      "epoch 538: w = 1.981, loss = 0.00054495\n",
      "epoch 540: w = 1.981, loss = 0.00053845\n",
      "epoch 542: w = 1.981, loss = 0.00053203\n",
      "epoch 544: w = 1.981, loss = 0.00052569\n",
      "epoch 546: w = 1.981, loss = 0.00051942\n",
      "epoch 548: w = 1.981, loss = 0.00051323\n",
      "epoch 550: w = 1.981, loss = 0.00050711\n",
      "epoch 552: w = 1.981, loss = 0.00050107\n",
      "epoch 554: w = 1.982, loss = 0.00049509\n",
      "epoch 556: w = 1.982, loss = 0.00048919\n",
      "epoch 558: w = 1.982, loss = 0.00048336\n",
      "epoch 560: w = 1.982, loss = 0.00047759\n",
      "epoch 562: w = 1.982, loss = 0.00047190\n",
      "epoch 564: w = 1.982, loss = 0.00046628\n",
      "epoch 566: w = 1.982, loss = 0.00046072\n",
      "epoch 568: w = 1.982, loss = 0.00045522\n",
      "epoch 570: w = 1.982, loss = 0.00044979\n",
      "epoch 572: w = 1.983, loss = 0.00044443\n",
      "epoch 574: w = 1.983, loss = 0.00043914\n",
      "epoch 576: w = 1.983, loss = 0.00043390\n",
      "epoch 578: w = 1.983, loss = 0.00042873\n",
      "epoch 580: w = 1.983, loss = 0.00042362\n",
      "epoch 582: w = 1.983, loss = 0.00041857\n",
      "epoch 584: w = 1.983, loss = 0.00041358\n",
      "epoch 586: w = 1.983, loss = 0.00040864\n",
      "epoch 588: w = 1.983, loss = 0.00040377\n",
      "epoch 590: w = 1.983, loss = 0.00039896\n",
      "epoch 592: w = 1.984, loss = 0.00039420\n",
      "epoch 594: w = 1.984, loss = 0.00038950\n",
      "epoch 596: w = 1.984, loss = 0.00038486\n",
      "epoch 598: w = 1.984, loss = 0.00038027\n",
      "epoch 600: w = 1.984, loss = 0.00037574\n",
      "epoch 602: w = 1.984, loss = 0.00037126\n",
      "epoch 604: w = 1.984, loss = 0.00036683\n",
      "epoch 606: w = 1.984, loss = 0.00036246\n",
      "epoch 608: w = 1.984, loss = 0.00035814\n",
      "epoch 610: w = 1.984, loss = 0.00035387\n",
      "epoch 612: w = 1.984, loss = 0.00034965\n",
      "epoch 614: w = 1.985, loss = 0.00034548\n",
      "epoch 616: w = 1.985, loss = 0.00034137\n",
      "epoch 618: w = 1.985, loss = 0.00033730\n",
      "epoch 620: w = 1.985, loss = 0.00033327\n",
      "epoch 622: w = 1.985, loss = 0.00032930\n",
      "epoch 624: w = 1.985, loss = 0.00032537\n",
      "epoch 626: w = 1.985, loss = 0.00032149\n",
      "epoch 628: w = 1.985, loss = 0.00031766\n",
      "epoch 630: w = 1.985, loss = 0.00031387\n",
      "epoch 632: w = 1.985, loss = 0.00031014\n",
      "epoch 634: w = 1.985, loss = 0.00030643\n",
      "epoch 636: w = 1.986, loss = 0.00030278\n",
      "epoch 638: w = 1.986, loss = 0.00029917\n",
      "epoch 640: w = 1.986, loss = 0.00029561\n",
      "epoch 642: w = 1.986, loss = 0.00029208\n",
      "epoch 644: w = 1.986, loss = 0.00028860\n",
      "epoch 646: w = 1.986, loss = 0.00028516\n",
      "epoch 648: w = 1.986, loss = 0.00028176\n",
      "epoch 650: w = 1.986, loss = 0.00027840\n",
      "epoch 652: w = 1.986, loss = 0.00027508\n",
      "epoch 654: w = 1.986, loss = 0.00027180\n",
      "epoch 656: w = 1.986, loss = 0.00026856\n",
      "epoch 658: w = 1.986, loss = 0.00026536\n",
      "epoch 660: w = 1.987, loss = 0.00026220\n",
      "epoch 662: w = 1.987, loss = 0.00025907\n",
      "epoch 664: w = 1.987, loss = 0.00025598\n",
      "epoch 666: w = 1.987, loss = 0.00025293\n",
      "epoch 668: w = 1.987, loss = 0.00024992\n",
      "epoch 670: w = 1.987, loss = 0.00024694\n",
      "epoch 672: w = 1.987, loss = 0.00024399\n",
      "epoch 674: w = 1.987, loss = 0.00024108\n",
      "epoch 676: w = 1.987, loss = 0.00023821\n",
      "epoch 678: w = 1.987, loss = 0.00023537\n",
      "epoch 680: w = 1.987, loss = 0.00023256\n",
      "epoch 682: w = 1.987, loss = 0.00022979\n",
      "epoch 684: w = 1.987, loss = 0.00022705\n",
      "epoch 686: w = 1.988, loss = 0.00022434\n",
      "epoch 688: w = 1.988, loss = 0.00022167\n",
      "epoch 690: w = 1.988, loss = 0.00021902\n",
      "epoch 692: w = 1.988, loss = 0.00021641\n",
      "epoch 694: w = 1.988, loss = 0.00021383\n",
      "epoch 696: w = 1.988, loss = 0.00021128\n",
      "epoch 698: w = 1.988, loss = 0.00020876\n",
      "epoch 700: w = 1.988, loss = 0.00020628\n",
      "epoch 702: w = 1.988, loss = 0.00020382\n",
      "epoch 704: w = 1.988, loss = 0.00020139\n",
      "epoch 706: w = 1.988, loss = 0.00019899\n",
      "epoch 708: w = 1.988, loss = 0.00019661\n",
      "epoch 710: w = 1.988, loss = 0.00019427\n",
      "epoch 712: w = 1.989, loss = 0.00019195\n",
      "epoch 714: w = 1.989, loss = 0.00018966\n",
      "epoch 716: w = 1.989, loss = 0.00018740\n",
      "epoch 718: w = 1.989, loss = 0.00018517\n",
      "epoch 720: w = 1.989, loss = 0.00018296\n",
      "epoch 722: w = 1.989, loss = 0.00018078\n",
      "epoch 724: w = 1.989, loss = 0.00017862\n",
      "epoch 726: w = 1.989, loss = 0.00017649\n",
      "epoch 728: w = 1.989, loss = 0.00017439\n",
      "epoch 730: w = 1.989, loss = 0.00017231\n",
      "epoch 732: w = 1.989, loss = 0.00017026\n",
      "epoch 734: w = 1.989, loss = 0.00016823\n",
      "epoch 736: w = 1.989, loss = 0.00016622\n",
      "epoch 738: w = 1.989, loss = 0.00016424\n",
      "epoch 740: w = 1.989, loss = 0.00016228\n",
      "epoch 742: w = 1.989, loss = 0.00016035\n",
      "epoch 744: w = 1.990, loss = 0.00015844\n",
      "epoch 746: w = 1.990, loss = 0.00015655\n",
      "epoch 748: w = 1.990, loss = 0.00015468\n",
      "epoch 750: w = 1.990, loss = 0.00015284\n",
      "epoch 752: w = 1.990, loss = 0.00015101\n",
      "epoch 754: w = 1.990, loss = 0.00014921\n",
      "epoch 756: w = 1.990, loss = 0.00014743\n",
      "epoch 758: w = 1.990, loss = 0.00014568\n",
      "epoch 760: w = 1.990, loss = 0.00014394\n",
      "epoch 762: w = 1.990, loss = 0.00014222\n",
      "epoch 764: w = 1.990, loss = 0.00014053\n",
      "epoch 766: w = 1.990, loss = 0.00013885\n",
      "epoch 768: w = 1.990, loss = 0.00013720\n",
      "epoch 770: w = 1.990, loss = 0.00013556\n",
      "epoch 772: w = 1.990, loss = 0.00013395\n",
      "epoch 774: w = 1.990, loss = 0.00013235\n",
      "epoch 776: w = 1.991, loss = 0.00013077\n",
      "epoch 778: w = 1.991, loss = 0.00012921\n",
      "epoch 780: w = 1.991, loss = 0.00012767\n",
      "epoch 782: w = 1.991, loss = 0.00012615\n",
      "epoch 784: w = 1.991, loss = 0.00012464\n",
      "epoch 786: w = 1.991, loss = 0.00012316\n",
      "epoch 788: w = 1.991, loss = 0.00012169\n",
      "epoch 790: w = 1.991, loss = 0.00012024\n",
      "epoch 792: w = 1.991, loss = 0.00011881\n",
      "epoch 794: w = 1.991, loss = 0.00011739\n",
      "epoch 796: w = 1.991, loss = 0.00011599\n",
      "epoch 798: w = 1.991, loss = 0.00011461\n",
      "epoch 800: w = 1.991, loss = 0.00011324\n",
      "epoch 802: w = 1.991, loss = 0.00011189\n",
      "epoch 804: w = 1.991, loss = 0.00011056\n",
      "epoch 806: w = 1.991, loss = 0.00010924\n",
      "epoch 808: w = 1.991, loss = 0.00010794\n",
      "epoch 810: w = 1.991, loss = 0.00010665\n",
      "epoch 812: w = 1.991, loss = 0.00010538\n",
      "epoch 814: w = 1.992, loss = 0.00010412\n",
      "epoch 816: w = 1.992, loss = 0.00010288\n",
      "epoch 818: w = 1.992, loss = 0.00010165\n",
      "epoch 820: w = 1.992, loss = 0.00010044\n",
      "epoch 822: w = 1.992, loss = 0.00009925\n",
      "epoch 824: w = 1.992, loss = 0.00009806\n",
      "epoch 826: w = 1.992, loss = 0.00009689\n",
      "epoch 828: w = 1.992, loss = 0.00009574\n",
      "epoch 830: w = 1.992, loss = 0.00009460\n",
      "epoch 832: w = 1.992, loss = 0.00009347\n",
      "epoch 834: w = 1.992, loss = 0.00009235\n",
      "epoch 836: w = 1.992, loss = 0.00009125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 838: w = 1.992, loss = 0.00009016\n",
      "epoch 840: w = 1.992, loss = 0.00008909\n",
      "epoch 842: w = 1.992, loss = 0.00008803\n",
      "epoch 844: w = 1.992, loss = 0.00008698\n",
      "epoch 846: w = 1.992, loss = 0.00008594\n",
      "epoch 848: w = 1.992, loss = 0.00008492\n",
      "epoch 850: w = 1.992, loss = 0.00008390\n",
      "epoch 852: w = 1.992, loss = 0.00008290\n",
      "epoch 854: w = 1.992, loss = 0.00008192\n",
      "epoch 856: w = 1.993, loss = 0.00008094\n",
      "epoch 858: w = 1.993, loss = 0.00007997\n",
      "epoch 860: w = 1.993, loss = 0.00007902\n",
      "epoch 862: w = 1.993, loss = 0.00007808\n",
      "epoch 864: w = 1.993, loss = 0.00007715\n",
      "epoch 866: w = 1.993, loss = 0.00007623\n",
      "epoch 868: w = 1.993, loss = 0.00007532\n",
      "epoch 870: w = 1.993, loss = 0.00007442\n",
      "epoch 872: w = 1.993, loss = 0.00007354\n",
      "epoch 874: w = 1.993, loss = 0.00007266\n",
      "epoch 876: w = 1.993, loss = 0.00007179\n",
      "epoch 878: w = 1.993, loss = 0.00007094\n",
      "epoch 880: w = 1.993, loss = 0.00007009\n",
      "epoch 882: w = 1.993, loss = 0.00006926\n",
      "epoch 884: w = 1.993, loss = 0.00006843\n",
      "epoch 886: w = 1.993, loss = 0.00006761\n",
      "epoch 888: w = 1.993, loss = 0.00006681\n",
      "epoch 890: w = 1.993, loss = 0.00006601\n",
      "epoch 892: w = 1.993, loss = 0.00006522\n",
      "epoch 894: w = 1.993, loss = 0.00006445\n",
      "epoch 896: w = 1.993, loss = 0.00006368\n",
      "epoch 898: w = 1.993, loss = 0.00006292\n",
      "epoch 900: w = 1.993, loss = 0.00006217\n",
      "epoch 902: w = 1.993, loss = 0.00006143\n",
      "epoch 904: w = 1.994, loss = 0.00006070\n",
      "epoch 906: w = 1.994, loss = 0.00005997\n",
      "epoch 908: w = 1.994, loss = 0.00005926\n",
      "epoch 910: w = 1.994, loss = 0.00005855\n",
      "epoch 912: w = 1.994, loss = 0.00005785\n",
      "epoch 914: w = 1.994, loss = 0.00005716\n",
      "epoch 916: w = 1.994, loss = 0.00005648\n",
      "epoch 918: w = 1.994, loss = 0.00005581\n",
      "epoch 920: w = 1.994, loss = 0.00005514\n",
      "epoch 922: w = 1.994, loss = 0.00005448\n",
      "epoch 924: w = 1.994, loss = 0.00005384\n",
      "epoch 926: w = 1.994, loss = 0.00005319\n",
      "epoch 928: w = 1.994, loss = 0.00005256\n",
      "epoch 930: w = 1.994, loss = 0.00005193\n",
      "epoch 932: w = 1.994, loss = 0.00005131\n",
      "epoch 934: w = 1.994, loss = 0.00005070\n",
      "epoch 936: w = 1.994, loss = 0.00005010\n",
      "epoch 938: w = 1.994, loss = 0.00004950\n",
      "epoch 940: w = 1.994, loss = 0.00004891\n",
      "epoch 942: w = 1.994, loss = 0.00004833\n",
      "epoch 944: w = 1.994, loss = 0.00004775\n",
      "epoch 946: w = 1.994, loss = 0.00004718\n",
      "epoch 948: w = 1.994, loss = 0.00004662\n",
      "epoch 950: w = 1.994, loss = 0.00004606\n",
      "epoch 952: w = 1.994, loss = 0.00004551\n",
      "epoch 954: w = 1.994, loss = 0.00004497\n",
      "epoch 956: w = 1.994, loss = 0.00004444\n",
      "epoch 958: w = 1.995, loss = 0.00004390\n",
      "epoch 960: w = 1.995, loss = 0.00004338\n",
      "epoch 962: w = 1.995, loss = 0.00004286\n",
      "epoch 964: w = 1.995, loss = 0.00004235\n",
      "epoch 966: w = 1.995, loss = 0.00004185\n",
      "epoch 968: w = 1.995, loss = 0.00004135\n",
      "epoch 970: w = 1.995, loss = 0.00004086\n",
      "epoch 972: w = 1.995, loss = 0.00004037\n",
      "epoch 974: w = 1.995, loss = 0.00003989\n",
      "epoch 976: w = 1.995, loss = 0.00003941\n",
      "epoch 978: w = 1.995, loss = 0.00003894\n",
      "epoch 980: w = 1.995, loss = 0.00003848\n",
      "epoch 982: w = 1.995, loss = 0.00003802\n",
      "epoch 984: w = 1.995, loss = 0.00003757\n",
      "epoch 986: w = 1.995, loss = 0.00003712\n",
      "epoch 988: w = 1.995, loss = 0.00003668\n",
      "epoch 990: w = 1.995, loss = 0.00003624\n",
      "epoch 992: w = 1.995, loss = 0.00003581\n",
      "epoch 994: w = 1.995, loss = 0.00003538\n",
      "epoch 996: w = 1.995, loss = 0.00003496\n",
      "epoch 998: w = 1.995, loss = 0.00003454\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    # forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # compute loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # backward pass\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # empty gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f\"epoch {epoch}: w = {w[0].item():.3f}, loss = {l:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction after training: f(5)=9.990\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction after training: f(5)={model(X_test).item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
