{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyU55g1y1DGax/F5b350Iy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AchrafAsh/ml_projects/blob/main/mnist_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Hqvpygtg9z"
      },
      "source": [
        "## TODO\n",
        "\n",
        "- [ ] Image augmentation (⚠️ don't rotate images as they might look like another digit)\n",
        "- [ ] Image processing: Normalization\n",
        "- [ ] Xavier weight initialization\n",
        "\n",
        "<br/>\n",
        "\n",
        "## Architecture\n",
        "1. 2D convolutional layer\n",
        "2. Relu\n",
        "3. Dense layer\n",
        "4. Sigmoid\n",
        "\n",
        "Conv2D (same for max pooling): \n",
        "$$ H_{out} = \\frac{H_{in} + 2 \\times padding[0] - dilation[0] \\times (kernel\\_size[0] - 1) - 1}{stride[0]} + 1 $$\n",
        "\n",
        "$$ W_{out} = \\frac{W_{in} + 2  \\times padding[1] - dilation[1] \\times (kernel\\_size[1] - 1) - 1}{stride[1]} + 1 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N58po09Dr1cp"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Using CNN and image processing to solve the classic MNIST classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAMD1b1asByU"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pyeUUa6xX-7"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "The data is in a csv file where each column is a pixel of the image (gray scale).\n",
        "Let's create a custom dataset to rebuild the image from the csv data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCFST4tp3ZgQ"
      },
      "source": [
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, root_dir, filename):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(os.path.join(root_dir, filename), nrows=100)\n",
        "        self.labels = self.df[\"label\"]\n",
        "        self.images = self.df.iloc[:, 1:]\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_image(pix_arr):\n",
        "        \"\"\"Takes a 1D array of pixels and returns a 2D tensor\"\"\"\n",
        "        image = torch.tensor(pix_arr).float()\n",
        "        image = torch.reshape(image, (1, 28, 28))\n",
        "        return image\n",
        "    \n",
        "    @staticmethod\n",
        "    def show_image(tensor):\n",
        "        tensor = torch.reshape(tensor, (28,28))\n",
        "        npimg = tensor.numpy()\n",
        "        plt.imshow(npimg)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = torch.tensor(self.labels.loc[idx])\n",
        "        image = self.build_image(self.images.loc[idx])\n",
        "        return image, label"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAHwa6Aq6NNH"
      },
      "source": [
        "def get_loader(batch_size, train=True, num_workers=1, pin_memory=True):\n",
        "    drive.mount(\"/content/gdrive\")\n",
        "\n",
        "    root_dir = \"gdrive/My Drive/Projects/data/mnist\"\n",
        "    if train: filename = \"train.csv\"\n",
        "    else: filename = \"test.csv\"\n",
        "\n",
        "    dataset = MNISTDataset(root_dir, filename)\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            num_workers=num_workers, shuffle=train,\n",
        "                            pin_memory=pin_memory)\n",
        "\n",
        "    return dataset, dataloader"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID1_aY2N_Pye"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8-sjwHTsFTF"
      },
      "source": [
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=19, stride=1, padding=0) # output (batch_size, 1, )\n",
        "        self.dense = nn.Linear(in_features=10, out_features=1)\n",
        "    \n",
        "    def forward(self, image):\n",
        "        batch_size = image.shape[0]\n",
        "        output = F.relu(self.conv(image))\n",
        "        output = self.dense(output)\n",
        "        output = torch.reshape(output, (batch_size, 10))\n",
        "        output = F.softmax(output, dim=1)\n",
        "        return output\n",
        "        \n",
        "    def fit(self, dataset, num_epochs, lr):\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0\n",
        "            accuracy = 0\n",
        "\n",
        "            for _, (images, targets) in enumerate(dataset):\n",
        "                labels = self.forward(images)\n",
        "                loss = criterion(labels, targets)\n",
        "                accuracy = sum(torch.argmax(labels, dim=1) == targets)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                total_loss += loss\n",
        "            \n",
        "            if (epoch % 1 == 0): print(f\"epoch [{epoch+1} / {num_epochs}] | total loss: {total_loss} | accuracy: {accuracy / (len(dataset) * dataset.batch_size)}\")\n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fedEwLGG_YLw"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REFPDN12_cFu"
      },
      "source": [
        "# Parameters\n",
        "BATCH_SIZE=2\n",
        "NUM_EPOCHS=100\n",
        "LEARNING_RATE=0.1"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Eg3kPl_U2e"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "397Hx-wR_XL4",
        "outputId": "d9e528da-321a-4922-a9b8-b3f7ff104ea1"
      },
      "source": [
        "dataset, dataloader = get_loader(batch_size=BATCH_SIZE)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "OIfHQtkpAB-O",
        "outputId": "915c9c69-18da-441d-c909-f02d7b65df82"
      },
      "source": [
        "image, label = dataset[0]\n",
        "dataset.show_image(image)\n",
        "plt.title(label.item())"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANjUlEQVR4nO3dcayV9X3H8c9HQNhAV27VKwNWnCNLyJJic0O7ajpXU6cmCzbpSFln6GJKs5ZMuy7TuCZ1yf5wVuvsttrQScWm1TarRpaQto41M53GeKEMUDp1FFNukTtliVgtXOC7P+5jc5V7fud6znPOc+D7fiUn95zne57zfHPCh985z++c83NECMCZ76ymGwDQH4QdSIKwA0kQdiAJwg4kQdiBJAg7kARhxylsb7A9avuo7fua7gf1mN10AxhIP5P0t5L+QNKvNNwLakLYcYqIeEiSbI9IWtJwO6gJL+OBJAg7kARhB5Ig7EASnKDDKWzP1uS/jVmSZtmeJ+l4RBxvtjN0g5Ed0/mcpNcl3SzpT6rrn2u0I3TN/HgFkAMjO5AEYQeSIOxAEoQdSKKvU29ne27M0/x+HhJI5Rf6uY7FUU9X6yrstq+SdLcm52P/OSJuK91/nubrvb6im0MCKHgytrWsdfwy3vYsSf8k6WpJKySttb2i08cD0FvdvGdfJen5iNgXEcckPShpdT1tAahbN2FfLOmnU24fqLa9ie311a+ejE7oaBeHA9CNnp+Nj4iNETESESNzNLfXhwPQQjdhH5O0dMrtJdU2AAOom7A/JWm57Ytsny3po5K21NMWgLp1PPUWEcdtb5D0PU1OvW2KiKdr6wxArbqaZ4+IrZK21tQLgB7i47JAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJNHXJZuBflr4n0Mtaw9e9O/Ffd/9d58q1i+8+/GOemoSIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME8O05bw0+cW6x/eWnrBYYnYk5xX0dHLQ20rsJue7+kI5JOSDoeESN1NAWgfnWM7L8fES/V8DgAeoj37EAS3YY9JH3f9nbb66e7g+31tkdtj07oaJeHA9Cpbl/GXxYRY7YvkPSo7R9HxGNT7xARGyVtlKRzPXQGnvYATg9djewRMVb9HZf0sKRVdTQFoH4dh932fNvnvHFd0pWS9tTVGIB6dfMyfljSw7bfeJxvRsR3a+kKkLTv9t8t1h9ccmexPtdzW9bet2Ntcd9fv688bp0oVgdTx2GPiH2S3l1jLwB6iKk3IAnCDiRB2IEkCDuQBGEHkuArrmjM4T8tT609sfaOYn3BWfOK9S+8vKJlbfjj5e9unXjllWL9dMTIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM+Onpr127/Vsrb6Mz8o7vtrbebRdx0rf9H0kTs+2LL2jpefKO57JmJkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkmGdHVyauLC/c+8E7/6Nl7S+GftzVsT9x+w3F+vn355tLL2FkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkmGdH0aE/f3+xvv2mfyzWTypa1p6dOFbc9/pnrivWFz28r1g/Xqzm03Zkt73J9rjtPVO2Ddl+1PZz1d+FvW0TQLdm8jL+PklXvWXbzZK2RcRySduq2wAGWNuwR8Rjkg6/ZfNqSZur65slXVtzXwBq1ul79uGIOFhdf1HScKs72l4vab0kzdOvdng4AN3q+mx8RITU+ixMRGyMiJGIGJmjud0eDkCHOg37IduLJKn6O15fSwB6odOwb5G0rrq+TtIj9bQDoFfavme3/YCkyyWdZ/uApM9Luk3St21fL+kFSWt62SR6Z/ay3yjWP7b+ez079h+NfqJYX/qRPcU68+hvT9uwR8TaFqUrau4FQA/xcVkgCcIOJEHYgSQIO5AEYQeS4CuuZ7hZwxcU6x/4173F+o0Ln21zBBerPzn+i5a1+VvPafPYqBMjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTz7me7cBcVyt8smt3Pje/6wZW3oZZZU7idGdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Ignn2M8DsJYtb1lb9S3ke/aw230dv5zMH31usx+utv8+O/mJkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkmGc/A4x/ZX7L2i3n7S7ue7LNY9/ws0uL9Z/8Xnm8OPnaa22OgH5pO7Lb3mR73PaeKdtutT1me2d1uaa3bQLo1kxext8n6apptt8VESury9Z62wJQt7Zhj4jHJB3uQy8AeqibE3QbbO+qXuYvbHUn2+ttj9oendDRLg4HoBudhv0eSRdLWinpoKQ7W90xIjZGxEhEjMzR3A4PB6BbHYU9Ig5FxImIOCnpq5JW1dsWgLp1FHbbi6bc/LCkPa3uC2AwtJ1nt/2ApMslnWf7gKTPS7rc9kpJIWm/pE/2sMf0St9Xl6QPLe78t99fPVk+j7L9S5cU6+94jd9+P120DXtErJ1m87096AVAD/FxWSAJwg4kQdiBJAg7kARhB5LgK64DYPa7lhbr53zz58X631zwo5a1l068Xtz36jv+qlgf/vrjxTpOH4zsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE8+wD4IW15Xn2Hy37h44f+6ax8g//Dn+JefQsGNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnm2ftg/FPvL9Yf+rMvtHmEecXqhrHLWtZe/thQm8d+pU0dZwpGdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IYiZLNi+VdL+kYU0u0bwxIu62PSTpW5KWaXLZ5jUR8X+9a3VwzTr//GL9L2/4VrF+0ezyPHo7O+5Z2bI2tI8llTFpJiP7cUmfjYgVkt4n6dO2V0i6WdK2iFguaVt1G8CAahv2iDgYETuq60ck7ZW0WNJqSZuru22WdG2vmgTQvbf1nt32MkmXSHpS0nBEHKxKL2ryZT6AATXjsNteIOk7km6MiDd9oDoiQpPv56fbb73tUdujEzraVbMAOjejsNueo8mgfyMiHqo2H7K9qKovkjQ+3b4RsTEiRiJiZI7m1tEzgA60DbttS7pX0t6I+OKU0hZJ66rr6yQ9Un97AOoyk6+4XirpOkm7be+stt0i6TZJ37Z9vaQXJK3pTYuDb+yPlxfraxZ8t6fHP3aue/r4ODO0DXtE/FBSq39NV9TbDoBe4RN0QBKEHUiCsANJEHYgCcIOJEHYgST4KekanDVRrk/EiWJ9jmcV60ejfIAjF7d+/AuLeyITRnYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJ59hpc8OXHi/Wvbbi4WJ9/Vvnnuu76ykeK9eV/Xz4+IDGyA2kQdiAJwg4kQdiBJAg7kARhB5Ig7EASzLP3wZYV7+xq/wvFPDq6x8gOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0m0DbvtpbZ/YPsZ20/bvqHafqvtMds7q8s1vW8XQKdm8qGa45I+GxE7bJ8jabvtR6vaXRFxR+/aA1CXtmGPiIOSDlbXj9jeK2lxrxsDUK+39Z7d9jJJl0h6stq0wfYu25tsL2yxz3rbo7ZHJ1T++SUAvTPjsNteIOk7km6MiFck3SPpYkkrNTny3zndfhGxMSJGImJkjubW0DKATswo7LbnaDLo34iIhyQpIg5FxImIOCnpq5JW9a5NAN2aydl4S7pX0t6I+OKU7Yum3O3DkvbU3x6AuszkbPylkq6TtNv2zmrbLZLW2l4pKSTtl/TJnnQIoBYzORv/Q0meprS1/nYA9AqfoAOSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiThiOjfwez/lfTClE3nSXqpbw28PYPa26D2JdFbp+rs7V0Rcf50hb6G/ZSD26MRMdJYAwWD2tug9iXRW6f61Rsv44EkCDuQRNNh39jw8UsGtbdB7Uuit071pbdG37MD6J+mR3YAfULYgSQaCbvtq2z/t+3nbd/cRA+t2N5ve3e1DPVow71ssj1ue8+UbUO2H7X9XPV32jX2GuptIJbxLiwz3uhz1/Ty531/z257lqRnJX1I0gFJT0laGxHP9LWRFmzvlzQSEY1/AMP2ByS9Kun+iPidatvtkg5HxG3Vf5QLI+KmAentVkmvNr2Md7Va0aKpy4xLulbSx9Xgc1foa4368Lw1MbKvkvR8ROyLiGOSHpS0uoE+Bl5EPCbp8Fs2r5a0ubq+WZP/WPquRW8DISIORsSO6voRSW8sM97oc1foqy+aCPtiST+dcvuABmu995D0fdvbba9vuplpDEfEwer6i5KGm2xmGm2X8e6ntywzPjDPXSfLn3eLE3Snuiwi3iPpakmfrl6uDqSYfA82SHOnM1rGu1+mWWb8l5p87jpd/rxbTYR9TNLSKbeXVNsGQkSMVX/HJT2swVuK+tAbK+hWf8cb7ueXBmkZ7+mWGdcAPHdNLn/eRNifkrTc9kW2z5b0UUlbGujjFLbnVydOZHu+pCs1eEtRb5G0rrq+TtIjDfbyJoOyjHerZcbV8HPX+PLnEdH3i6RrNHlG/n8k/XUTPbTo6zcl/Vd1ebrp3iQ9oMmXdROaPLdxvaR3Stom6TlJ/yZpaIB6+7qk3ZJ2aTJYixrq7TJNvkTfJWlndbmm6eeu0Fdfnjc+LgskwQk6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUji/wF6VOjTh3brrAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zztY1PqBOpH"
      },
      "source": [
        "model = MNISTClassifier()"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AMaPl-OuB1xa",
        "outputId": "a3f80860-8b7f-41d6-97db-76c0aa20f27b"
      },
      "source": [
        "model.fit(dataloader, num_epochs=NUM_EPOCHS, lr=LEARNING_RATE)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch [1 / 100] | total loss: 115.20317840576172 | accuracy: 0.0\n",
            "epoch [2 / 100] | total loss: 115.20318603515625 | accuracy: 0.0\n",
            "epoch [3 / 100] | total loss: 115.20317840576172 | accuracy: 0.0\n",
            "epoch [4 / 100] | total loss: 115.20317077636719 | accuracy: 0.0\n",
            "epoch [5 / 100] | total loss: 115.20309448242188 | accuracy: 0.0\n",
            "epoch [6 / 100] | total loss: 114.91067504882812 | accuracy: 0.0\n",
            "epoch [7 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [8 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [9 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [10 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [11 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [12 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [13 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [14 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [15 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [16 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [17 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [18 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [19 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [20 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [21 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [22 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [23 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [24 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [25 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [26 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [27 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [28 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [29 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [30 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [31 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [32 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [33 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [34 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [35 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [36 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [37 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [38 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n",
            "epoch [39 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [40 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [41 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [42 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [43 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [44 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [45 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [46 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [47 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [48 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [49 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [50 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [51 / 100] | total loss: 115.12918853759766 | accuracy: 0.0\n",
            "epoch [52 / 100] | total loss: 115.12918853759766 | accuracy: 0.009999999776482582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-2379e3d771e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-105-3acb3173e1e0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, num_epochs, lr)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-3acb3173e1e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}