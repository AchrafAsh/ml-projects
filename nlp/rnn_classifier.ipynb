{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rnn_dataset import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.whh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.wxh = nn.Linear(input_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state): # x (batch_size, input_size)\n",
    "        return torch.sigmoid(self.whh(hidden_state) + self.wxh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, output_size=2):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn1 = RNNCell(input_size, hidden_size)\n",
    "        self.rnn2 = RNNCell(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "#         self.whh = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.wxh = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "    \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state): # x (seq_len, batch_size, input_size)\n",
    "        h1 = hidden_state\n",
    "        h2 = hidden_state\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            h1 = self.rnn1(x[i], h1)\n",
    "            h2 = self.rnn2(h1, h2)\n",
    "        \n",
    "        output = F.softmax(self.fc(h2), dim=1)\n",
    "        return output, hidden_state\n",
    "    \n",
    "    def fit(self, dataset, batch_size, epochs, lr=0.001):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss() # ignore_index=pad_idx?\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for idx, (texts, sentiments) in enumerate(dataset):\n",
    "                hidden_state = self.init_hidden_state(batch_size=batch_size)\n",
    "                \n",
    "                # forward\n",
    "                for i in range(texts.shape[0]):\n",
    "                    output, hidden_state = self.forward(texts[i], hidden_state)\n",
    "                \n",
    "                loss = criterion(output, sentiments)\n",
    "                \n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
    "                \n",
    "                # gradient descent or Adam step\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss\n",
    "            \n",
    "            if (epoch % 10 == 0): print(f\"epoch [{epoch+1} / {epochs}] | total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "HIDDEN_SIZE=256\n",
    "OUTPUT_SIZE=2\n",
    "LR=0.5\n",
    "NUM_EPOCHS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATA\n",
    "dataloader,dataset = get_loader(\"../data/\", \"small_sentiments.csv\", batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentClassifier(input_size=dataset.vocab_size, hidden_size=HIDDEN_SIZE,\n",
    "                                output_size=OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1 / 200] | total loss: 1.3130816221237183\n",
      "epoch [11 / 200] | total loss: 1.313055157661438\n",
      "epoch [21 / 200] | total loss: 1.3130193948745728\n",
      "epoch [31 / 200] | total loss: 1.3129692077636719\n",
      "epoch [41 / 200] | total loss: 1.3128929138183594\n",
      "epoch [51 / 200] | total loss: 1.3127641677856445\n",
      "epoch [61 / 200] | total loss: 1.3125016689300537\n",
      "epoch [71 / 200] | total loss: 1.3116923570632935\n",
      "epoch [81 / 200] | total loss: 1.2842903137207031\n",
      "epoch [91 / 200] | total loss: 0.31467482447624207\n",
      "epoch [101 / 200] | total loss: 0.31396782398223877\n",
      "epoch [111 / 200] | total loss: 0.3137337267398834\n",
      "epoch [121 / 200] | total loss: 0.3136162757873535\n",
      "epoch [131 / 200] | total loss: 0.31354570388793945\n",
      "epoch [141 / 200] | total loss: 0.31349849700927734\n",
      "epoch [151 / 200] | total loss: 0.3134647011756897\n",
      "epoch [161 / 200] | total loss: 0.31343942880630493\n",
      "epoch [171 / 200] | total loss: 0.3134196400642395\n",
      "epoch [181 / 200] | total loss: 0.3134038746356964\n",
      "epoch [191 / 200] | total loss: 0.3133908808231354\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "classifier.fit(dataloader, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(classifier, vocab, sentence):\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    def one_hot_tensor(idx):\n",
    "        tensor = [0] * vocab_size\n",
    "        tensor[idx] = 1\n",
    "        return tensor\n",
    "    \n",
    "    encoded_text = []\n",
    "    encoded_text.append([one_hot_tensor(vocab.stoi[\"<SOS>\"])])\n",
    "    encoded_text += [[one_hot_tensor(encoded_token)]\n",
    "                     for encoded_token in vocab.encode(sentence)]\n",
    "    encoded_text.append([one_hot_tensor(vocab.stoi[\"<EOS>\"])])\n",
    "\n",
    "    encoded_text = torch.tensor(encoded_text).float()\n",
    "    print(encoded_text.shape)\n",
    "    h0 = classifier.init_hidden_state(batch_size=1)\n",
    "\n",
    "    for i in range(encoded_text.shape[0]):\n",
    "        output, h0 = classifier(encoded_text[i], h0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9.9978e-01, 2.2009e-04]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()\n",
    "get_sentiment(classifier, dataset.vocab, \"I love you\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
