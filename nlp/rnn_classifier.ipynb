{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "rnn_classifier.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AchrafAsh/ml_projects/blob/main/nlp/rnn_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYPmttnHE6iU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Pc7T7UFZHs"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import spacy  # for tokenization\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence  # padding of every batch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "\n",
        "    def __len__(self): return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_en(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        frequencies = {}  # store the frequency of each word encountered\n",
        "        idx = 4  # 0, 1, 2 and 3 are already set\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word in self.tokenizer_en(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "        return\n",
        "\n",
        "    def toint(self, word):\n",
        "        if word in self.stoi:\n",
        "            return self.stoi[word]\n",
        "        else:\n",
        "            return self.stoi[\"<UNK>\"]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx in self.itos:\n",
        "            return self.itos[idx]\n",
        "        else:\n",
        "            return \"<UNK>\"\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        tokens = self.tokenizer_en(sentence)\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokens\n",
        "        ]\n",
        "\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, root_dir, filename, freq_threshold=1):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(os.path.join(root_dir, filename))\n",
        "\n",
        "        self.sentiments = self.df[\"sentiment\"]\n",
        "        self.texts = self.df[\"text\"]\n",
        "\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.texts.tolist())\n",
        "\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def one_hot_tensor(self, idx):\n",
        "        tensor = np.zeros(self.vocab_size)\n",
        "        tensor[idx] = 1\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded_text = [self.one_hot_tensor(self.vocab.stoi[\"<SOS>\"])]\n",
        "        encoded_text += [self.one_hot_tensor(encoded_token)\n",
        "                         for encoded_token in self.vocab.encode(self.texts[idx])]\n",
        "        encoded_text.append(self.one_hot_tensor(self.vocab.stoi[\"<EOS>\"]))\n",
        "\n",
        "        return {\"text\": torch.tensor(encoded_text).float(), \"sentiment\": torch.tensor(self.sentiments[idx])}\n",
        "\n",
        "\n",
        "class CollateBatch:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        sentiments = [item[\"sentiment\"] for item in batch]\n",
        "        texts = [item[\"text\"] for item in batch]\n",
        "        texts = pad_sequence(texts, batch_first=False,\n",
        "                             padding_value=self.pad_idx)\n",
        "\n",
        "        return texts, torch.tensor(sentiments)\n",
        "\n",
        "\n",
        "def get_loader(root_dir, filename, batch_size=10, num_workers=1, shuffle=True, pin_memory=True):\n",
        "    dataset = SentimentDataset(root_dir, filename)\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers,\n",
        "                        shuffle=shuffle, pin_memory=pin_memory, collate_fn=CollateBatch(pad_idx=pad_idx))\n",
        "    return loader, dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAKqgN7ME6iZ"
      },
      "source": [
        "class RNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RNNCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.whh = nn.Linear(hidden_size, hidden_size)\n",
        "        self.wxh = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x, hidden_state): # x (batch_size, input_size)\n",
        "        return self.relu(self.whh(hidden_state) + self.wxh(x))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNCPdaJCE6ia"
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, output_size=2):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn1 = RNNCell(input_size, hidden_size)\n",
        "        self.rnn2 = RNNCell(hidden_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "#         self.whh = nn.Linear(hidden_size, hidden_size)\n",
        "#         self.wxh = nn.Linear(input_size, hidden_size)\n",
        "        \n",
        "    \n",
        "    def init_hidden_state(self, batch_size):\n",
        "        return torch.zeros(batch_size, self.hidden_size)\n",
        "    \n",
        "    def forward(self, x, hidden_state): # x (seq_len, batch_size, input_size)\n",
        "        h1 = hidden_state\n",
        "        h2 = hidden_state\n",
        "\n",
        "        for i in range(x.shape[0]):\n",
        "            h1 = self.rnn1(x[i], h1)\n",
        "            h2 = self.rnn2(h1, h2)\n",
        "        \n",
        "        output = self.fc(h2)\n",
        "        return output, hidden_state\n",
        "    \n",
        "    def fit(self, dataset, batch_size, epochs, lr=0.001):\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss() # ignore_index=pad_idx?\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for idx, (texts, sentiments) in enumerate(dataset):\n",
        "                hidden_state = self.init_hidden_state(batch_size=batch_size)\n",
        "                \n",
        "                # forward\n",
        "                for i in range(texts.shape[0]):\n",
        "                    output, hidden_state = self.forward(texts[i], hidden_state)\n",
        "                \n",
        "                loss = criterion(output, sentiments)\n",
        "                \n",
        "                # backward\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
        "                \n",
        "                # gradient descent or Adam step\n",
        "                optimizer.step()\n",
        "                \n",
        "                total_loss += loss\n",
        "            \n",
        "            if (epoch % 10 == 0): print(f\"epoch [{epoch+1} / {epochs}] | total loss: {total_loss}\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snYKYijaE6ib"
      },
      "source": [
        "BATCH_SIZE=1\n",
        "HIDDEN_SIZE=124\n",
        "OUTPUT_SIZE=2\n",
        "LR=0.001\n",
        "NUM_EPOCHS=200"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubjPywEhE6ic"
      },
      "source": [
        "# LOADING THE DATA\n",
        "dataloader,dataset = get_loader(\".\", \"small_sentiments.csv\", batch_size=BATCH_SIZE)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJoJVf8YE6ic"
      },
      "source": [
        "classifier = SentimentClassifier(input_size=dataset.vocab_size, hidden_size=HIDDEN_SIZE,\n",
        "                                output_size=OUTPUT_SIZE)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "o_7u40LbE6ic",
        "outputId": "4ae9b6e5-c17e-4c9b-d1a0-9ae85b2abbd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TRAINING\n",
        "classifier.fit(dataloader, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, lr=LR)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch [1 / 200] | total loss: 2.7797744274139404\n",
            "epoch [11 / 200] | total loss: 2.7797744274139404\n",
            "epoch [21 / 200] | total loss: 2.7797744274139404\n",
            "epoch [31 / 200] | total loss: 2.779665946960449\n",
            "epoch [41 / 200] | total loss: 2.7793493270874023\n",
            "epoch [51 / 200] | total loss: 2.7797741889953613\n",
            "epoch [61 / 200] | total loss: 2.7794530391693115\n",
            "epoch [71 / 200] | total loss: 2.7795591354370117\n",
            "epoch [81 / 200] | total loss: 2.7794530391693115\n",
            "epoch [91 / 200] | total loss: 2.779665470123291\n",
            "epoch [101 / 200] | total loss: 2.779665231704712\n",
            "epoch [111 / 200] | total loss: 2.7795591354370117\n",
            "epoch [121 / 200] | total loss: 2.7795588970184326\n",
            "epoch [131 / 200] | total loss: 2.7794528007507324\n",
            "epoch [141 / 200] | total loss: 2.7793490886688232\n",
            "epoch [151 / 200] | total loss: 2.779348850250244\n",
            "epoch [161 / 200] | total loss: 2.779348850250244\n",
            "epoch [171 / 200] | total loss: 2.7794525623321533\n",
            "epoch [181 / 200] | total loss: 2.779773473739624\n",
            "epoch [191 / 200] | total loss: 2.779773235321045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSq-jtfvE6id"
      },
      "source": [
        "def get_sentiment(classifier, vocab, sentence):\n",
        "    vocab_size = len(vocab)\n",
        "    \n",
        "    def one_hot_tensor(idx):\n",
        "        tensor = [0] * vocab_size\n",
        "        tensor[idx] = 1\n",
        "        return tensor\n",
        "    \n",
        "    encoded_text = []\n",
        "    encoded_text.append([one_hot_tensor(vocab.stoi[\"<SOS>\"])])\n",
        "    encoded_text += [[one_hot_tensor(encoded_token)]\n",
        "                     for encoded_token in vocab.encode(sentence)]\n",
        "    encoded_text.append([one_hot_tensor(vocab.stoi[\"<EOS>\"])])\n",
        "\n",
        "    encoded_text = torch.tensor(encoded_text).float()\n",
        "    print(encoded_text.shape)\n",
        "    h0 = classifier.init_hidden_state(batch_size=1)\n",
        "\n",
        "    for i in range(encoded_text.shape[0]):\n",
        "        output, h0 = classifier(encoded_text[i], h0)\n",
        "    return output"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x91gREaE6ie",
        "outputId": "14c1eb79-97aa-494a-d109-55ca805e737c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classifier.eval()\n",
        "get_sentiment(classifier, dataset.vocab, \"I hate you\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 1, 17])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0996, -0.0016]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}