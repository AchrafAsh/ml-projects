{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rnn_dataset import Vocabulary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence  # padding of every batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, root_dir, filename, freq_threshold=1):\n",
    "        self.root_dir = root_dir\n",
    "        self.sentences = []\n",
    "        with open(os.path.join(root_dir, filename)) as f:\n",
    "            line = True\n",
    "            while line:\n",
    "                line = f.readline()\n",
    "                if(line != \"\\n\"):\n",
    "                    self.sentences.append(line)\n",
    "\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.sentences)\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def __len__(self): return len(self.sentences)\n",
    "\n",
    "    def one_hot_tensor(self, idx):\n",
    "        tensor = np.zeros(self.vocab_size)\n",
    "        tensor[idx] = 1\n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded_text = [self.one_hot_tensor(self.vocab.stoi[\"<SOS>\"])]\n",
    "        encoded_text += [self.one_hot_tensor(encoded_token)\n",
    "                         for encoded_token in self.vocab.encode(self.sentences[idx])]\n",
    "        encoded_text.append(self.one_hot_tensor(self.vocab.stoi[\"<EOS>\"]))\n",
    "\n",
    "        return torch.tensor(encoded_text).float()\n",
    "\n",
    "\n",
    "class CollateBatch:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        texts = [item for item in batch]\n",
    "        texts = pad_sequence(texts, batch_first=False,\n",
    "                             padding_value=self.pad_idx)\n",
    "\n",
    "        return texts\n",
    "\n",
    "\n",
    "def get_loader(root_dir, filename, batch_size=10, num_workers=1, shuffle=True, pin_memory=True):\n",
    "    dataset = TextDataset(root_dir, filename)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                        shuffle=shuffle, pin_memory=pin_memory, collate_fn=CollateBatch(pad_idx=pad_idx))\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.whh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.wxh = nn.Linear(input_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state, prev_state):\n",
    "        state = self.whh(hidden_state) + self.wxh(x)\n",
    "        filter_state = torch.sigmoid(state)\n",
    "        new_state = prev_state * filter_state + torch.tanh(state) * filter_state\n",
    "        hidden_state = torch.tanh(new_state) * filter_state\n",
    "        return hidden_state, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.lstm = LSTMCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state, prev_state):\n",
    "        for i in range(x.shape[0]):\n",
    "            hidden_state, prev_state = self.lstm(x[i], hidden_state, prev_state)\n",
    "        \n",
    "        return F.softmax(self.fc(hidden_state), dim=1)\n",
    "\n",
    "    def fit(self, dataset, batch_size, epochs, lr=0.001):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.MSELoss() # ignore_index=pad_idx?\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for k, (sentence) in enumerate(dataset):\n",
    "                if(len(sentence) < 4): continue\n",
    "                total_loss = 0\n",
    "                for idx in range(2,len(sentence)):\n",
    "                    h0 = self.init_hidden_state(batch_size=batch_size)\n",
    "                                        \n",
    "                    # forward\n",
    "                    output = self.forward(sentence[0:idx], h0, h0)\n",
    "                    loss = criterion(output, sentence[idx])\n",
    "\n",
    "                    # backward\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
    "\n",
    "                    # gradient descent or Adam step\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss\n",
    "            \n",
    "                if k%20 == 0: print(f\"epoch: [{epoch+1} / {epochs}] | sentence: [{k} / {len(dataset)}] | total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(generator, vocab, sentence, horizon=1):\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    def one_hot_tensor(idx):\n",
    "        tensor = [0] * vocab_size\n",
    "        tensor[idx] = 1\n",
    "        return tensor\n",
    "    \n",
    "    # encode input sentence\n",
    "    encoded_text = []\n",
    "    encoded_text.append([one_hot_tensor(vocab.stoi[\"<SOS>\"])])\n",
    "    encoded_text += [[one_hot_tensor(encoded_token)]\n",
    "                     for encoded_token in vocab.encode(sentence)]\n",
    "\n",
    "    encoded_text = torch.tensor(encoded_text).float()\n",
    "    print(encoded_text.shape)\n",
    "    h0 = generator.init_hidden_state(batch_size=1)\n",
    "\n",
    "    new_words = []\n",
    "    for _ in range(horizon):\n",
    "        output = generator(encoded_text, h0, h0)\n",
    "        word_index = torch.argmax(output).item()\n",
    "        new_tensor = torch.tensor([[one_hot_tensor(word_index)]]).float()\n",
    "\n",
    "        encoded_text = torch.cat((encoded_text, new_tensor), dim=0)\n",
    "        new_words.append(vocab[word_index])\n",
    "        \n",
    "    return sentence + \" \" + \" \".join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "HIDDEN_SIZE=124\n",
    "LR=0.005\n",
    "NUM_EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader,dataset = get_loader(\"../data/\", \"text.txt\", batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LSTMGenerator(input_size=dataset.vocab_size, hidden_size=HIDDEN_SIZE,\n",
    "                                output_size=dataset.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [1 / 20] | sentence: [0 / 67] | total loss: 0.047557100653648376\n",
      "epoch: [1 / 20] | sentence: [20 / 67] | total loss: 0.04884263500571251\n",
      "epoch: [1 / 20] | sentence: [40 / 67] | total loss: 0.0359894335269928\n",
      "epoch: [1 / 20] | sentence: [60 / 67] | total loss: 0.08611826598644257\n",
      "epoch: [2 / 20] | sentence: [0 / 67] | total loss: 0.021850328892469406\n",
      "epoch: [2 / 20] | sentence: [20 / 67] | total loss: 0.02956286072731018\n",
      "epoch: [2 / 20] | sentence: [40 / 67] | total loss: 0.061696507036685944\n",
      "epoch: [2 / 20] | sentence: [60 / 67] | total loss: 0.02056482806801796\n",
      "epoch: [3 / 20] | sentence: [0 / 67] | total loss: 0.08740343898534775\n",
      "epoch: [3 / 20] | sentence: [20 / 67] | total loss: 0.08611826598644257\n",
      "epoch: [3 / 20] | sentence: [40 / 67] | total loss: 0.012854075990617275\n",
      "epoch: [3 / 20] | sentence: [60 / 67] | total loss: 0.07198044657707214\n",
      "epoch: [4 / 20] | sentence: [0 / 67] | total loss: 0.026991358026862144\n",
      "epoch: [4 / 20] | sentence: [20 / 67] | total loss: 0.021850328892469406\n",
      "epoch: [4 / 20] | sentence: [40 / 67] | total loss: 0.042419206351041794\n",
      "epoch: [4 / 20] | sentence: [60 / 67] | total loss: 0.07455194741487503\n",
      "epoch: [5 / 20] | sentence: [0 / 67] | total loss: 0.01671033538877964\n",
      "epoch: [5 / 20] | sentence: [20 / 67] | total loss: 0.03213438391685486\n",
      "epoch: [5 / 20] | sentence: [40 / 67] | total loss: 0.047557100653648376\n",
      "epoch: [5 / 20] | sentence: [60 / 67] | total loss: 0.010282271541655064\n",
      "epoch: [6 / 20] | sentence: [0 / 67] | total loss: 0.15553095936775208\n",
      "epoch: [6 / 20] | sentence: [20 / 67] | total loss: 0.07197915017604828\n",
      "epoch: [6 / 20] | sentence: [40 / 67] | total loss: 0.02313765324652195\n",
      "epoch: [6 / 20] | sentence: [60 / 67] | total loss: 0.03213484585285187\n",
      "epoch: [7 / 20] | sentence: [0 / 67] | total loss: 0.03470311313867569\n",
      "epoch: [7 / 20] | sentence: [20 / 67] | total loss: 0.061699654906988144\n",
      "epoch: [7 / 20] | sentence: [40 / 67] | total loss: 0.06298281252384186\n",
      "epoch: [7 / 20] | sentence: [60 / 67] | total loss: 0.06298287212848663\n",
      "epoch: [8 / 20] | sentence: [0 / 67] | total loss: 0.060413576662540436\n",
      "epoch: [8 / 20] | sentence: [20 / 67] | total loss: 0.047557100653648376\n",
      "epoch: [8 / 20] | sentence: [40 / 67] | total loss: 0.07197915017604828\n",
      "epoch: [9 / 20] | sentence: [0 / 67] | total loss: 0.04884263500571251\n",
      "epoch: [9 / 20] | sentence: [20 / 67] | total loss: 0.07840998470783234\n",
      "epoch: [9 / 20] | sentence: [40 / 67] | total loss: 0.056556347757577896\n",
      "epoch: [9 / 20] | sentence: [60 / 67] | total loss: 0.07455194741487503\n",
      "epoch: [10 / 20] | sentence: [0 / 67] | total loss: 0.02313765324652195\n",
      "epoch: [10 / 20] | sentence: [20 / 67] | total loss: 0.012852798216044903\n",
      "epoch: [10 / 20] | sentence: [40 / 67] | total loss: 0.06298281252384186\n",
      "epoch: [10 / 20] | sentence: [60 / 67] | total loss: 0.08097967505455017\n",
      "epoch: [11 / 20] | sentence: [0 / 67] | total loss: 0.047557100653648376\n",
      "epoch: [11 / 20] | sentence: [20 / 67] | total loss: 0.012852798216044903\n",
      "epoch: [11 / 20] | sentence: [40 / 67] | total loss: 0.03341939300298691\n",
      "epoch: [11 / 20] | sentence: [60 / 67] | total loss: 0.07197915017604828\n",
      "epoch: [12 / 20] | sentence: [0 / 67] | total loss: 0.012852798216044903\n",
      "epoch: [12 / 20] | sentence: [20 / 67] | total loss: 0.011567625217139721\n",
      "epoch: [12 / 20] | sentence: [40 / 67] | total loss: 0.047557100653648376\n",
      "epoch: [12 / 20] | sentence: [60 / 67] | total loss: 0.08740273863077164\n",
      "epoch: [13 / 20] | sentence: [0 / 67] | total loss: 0.01671033538877964\n",
      "epoch: [13 / 20] | sentence: [20 / 67] | total loss: 0.015424545854330063\n",
      "epoch: [13 / 20] | sentence: [40 / 67] | total loss: 0.05784338712692261\n",
      "epoch: [13 / 20] | sentence: [60 / 67] | total loss: 0.04884263500571251\n",
      "epoch: [14 / 20] | sentence: [0 / 67] | total loss: 0.09126310795545578\n",
      "epoch: [14 / 20] | sentence: [20 / 67] | total loss: 0.07455194741487503\n",
      "epoch: [14 / 20] | sentence: [40 / 67] | total loss: 0.02313765324652195\n",
      "epoch: [14 / 20] | sentence: [60 / 67] | total loss: 0.08740454912185669\n",
      "epoch: [15 / 20] | sentence: [0 / 67] | total loss: 0.04884423315525055\n",
      "epoch: [15 / 20] | sentence: [20 / 67] | total loss: 0.06555350124835968\n",
      "epoch: [15 / 20] | sentence: [40 / 67] | total loss: 0.03213340789079666\n",
      "epoch: [15 / 20] | sentence: [60 / 67] | total loss: 0.09126310795545578\n",
      "epoch: [16 / 20] | sentence: [0 / 67] | total loss: 0.017994968220591545\n",
      "epoch: [16 / 20] | sentence: [20 / 67] | total loss: 0.08740273863077164\n",
      "epoch: [16 / 20] | sentence: [40 / 67] | total loss: 0.01671033538877964\n",
      "epoch: [16 / 20] | sentence: [60 / 67] | total loss: 0.042419206351041794\n",
      "epoch: [17 / 20] | sentence: [0 / 67] | total loss: 0.042419206351041794\n",
      "epoch: [17 / 20] | sentence: [20 / 67] | total loss: 0.052696626633405685\n",
      "epoch: [17 / 20] | sentence: [40 / 67] | total loss: 0.03855853155255318\n",
      "epoch: [17 / 20] | sentence: [60 / 67] | total loss: 0.07840998470783234\n",
      "epoch: [18 / 20] | sentence: [0 / 67] | total loss: 0.07198049873113632\n",
      "epoch: [18 / 20] | sentence: [20 / 67] | total loss: 0.011567624285817146\n",
      "epoch: [18 / 20] | sentence: [40 / 67] | total loss: 0.064269058406353\n",
      "epoch: [18 / 20] | sentence: [60 / 67] | total loss: 0.021851513534784317\n",
      "epoch: [19 / 20] | sentence: [0 / 67] | total loss: 0.06298286467790604\n",
      "epoch: [19 / 20] | sentence: [20 / 67] | total loss: 0.04755709692835808\n",
      "epoch: [19 / 20] | sentence: [60 / 67] | total loss: 0.07198049873113632\n",
      "epoch: [20 / 20] | sentence: [0 / 67] | total loss: 0.05784176290035248\n",
      "epoch: [20 / 20] | sentence: [20 / 67] | total loss: 0.04884269833564758\n",
      "epoch: [20 / 20] | sentence: [40 / 67] | total loss: 0.011567624285817146\n",
      "epoch: [20 / 20] | sentence: [60 / 67] | total loss: 0.056556347757577896\n"
     ]
    }
   ],
   "source": [
    "generator.train()\n",
    "generator.fit(dataloader, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 777])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Do not think that or made curation onslaught or onslaught or onslaught or onslaught'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.eval()\n",
    "generate(generator, dataset.vocab, \"Do not think that\", horizon=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
